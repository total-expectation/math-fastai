{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# List of common misconceptions in basic probability theory\n",
    "This is just a list of common misconceptions.\n",
    "\n",
    "* **[Gambler's fallacy](https://en.wikipedia.org/wiki/Gambler's_fallacy)** = for events that are i.i.d, the independence make it so that the probability of future trials will not be affected by previous attempts, i.e. the probability of them will neither go up or down just because certain values were observed in previous attempts. One can think of this as the process not having any memory. This is important to know because in combination with **law of large numbers**, where the samples are drawn from an i.i.d random variable, one can mistakenly believe that the probability of an event is more likely in order to adjust the current sample average to move closer to the population mean when the number of trial increases. But the gamblers fallacy says that this is not the case; the probability of the future events are unaffected by the past.\n",
    "* **Random variable** is not pdf/pmf, it's a function that models randomness but is not a probability distribution, it just maps outcomes from sample space to real numbers. For it to be a probability distribution it has to sum to 1 and every output from it has to be non-negative, which is not necessarily the case in general (you can easily design a r.v that doesn't fulfill the requirements of a probability distribution).\n",
    "* Law of large numbers vs central limit theorem = law of large numbers deals with the sample mean converging with a very high probability to the expected value as the number of samples goes to infinity. Central limit theorem says that this sample mean of a group of i.i.d r.v converges to a gaussian distribution, doesn't matter what distribution they come from. It even works on approximately independent r.v.\n",
    "* Independence and mutual exclusivity is not the same thing. For instance, $P(A,B) = 0$ if A and B are only mutually exclusive, while $P(A,B) = P(A)P(B)$ if they are only independent. Another example is $P(A|\\neg B) = \\frac{P(A)}{1-P(B)}$ if A and B are mutually exclusive, while $P(A|\\neg B) = P(A)$ if A and B are independent.\n",
    "* A simple mistake to make is that covariance of zero means two variables are independent, which is not true. However, if two variables are independent then they have zero covariance. This is a common misconception among people I feel like, and it's a typical situation of the \"inverse does not hold\". Two indep variable implies covariance zero, but covariance zero does **not** imply variables are indep. The reason for this is simply because covariance can only capture linear relationships between variables, but if there are non-linear relationships, then the result it gives might be misleading."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8bcb2c3a649a24b4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
