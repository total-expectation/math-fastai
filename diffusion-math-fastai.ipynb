{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50dd2904bda71f97",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Diffusion math\n",
    "This notebook provides bits and pieces of good insights to aid in understanding the math of diffusion models, in particular the math will be focused on [DDPM](https://arxiv.org/pdf/2006.11239) (Denoising diffusion probabilistic models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e97ff81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441832bb1e8259d0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Prerequisites\n",
    "General maturity in probability theory is good and basic familiarity with integrals. You don't necessarily need to know how to solve integrals, but be familiar with what they mean conceptually and how to interpret them. Otherwise, the really relevant concepts are\n",
    "* Joint, conditional, marginal probability\n",
    "* Expectation, variance, standard deviation, covariance\n",
    "* Bayes' theorem\n",
    "* Univariate and multivariate gaussian\n",
    "* KL-divergence\n",
    "* Information theory (self-information + entropy)\n",
    "* Evidence lower bound (ELBO)\n",
    "* Basic understanding of VAE helps alot since diffusion can be seen as an extension of VAE, but is not necessary\n",
    "\n",
    "You can either google these or look at my notebooks for these concepts.\n",
    "I cover the following concepts in [probability-fastai-probability-theory-concepts](probability-fastai-probability-theory-concepts.ipynb)\n",
    "* Joint, conditional, marginal probability\n",
    "* Expectation, variance, standard deviation, covariance\n",
    "* Bayes' theorem\n",
    "* Univariate and multivariate gaussian\n",
    "\n",
    "And concepts below can be found in [probability-fastai-probabilistic-machine-learning-concepts](probability-fastai-probabilistic-machine-learning-concepts.ipynb)\n",
    "* KL-divergence\n",
    "* Information theory (self-information + entropy)\n",
    "* Evidence lower bound (ELBO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca133fde8bdacfe",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Math techniques\n",
    "These are techniques to be aware of when going through derivations of the diffusion math. I assume you already know the prequisites above, so I won't present any of those concepts here. The techniques presented in this section are complementary and are things you would see when reading diffusion math related to DDPM.\n",
    "\n",
    "Outline of the subsections\n",
    "* Log laws\n",
    "* Random division fact\n",
    "* Factoring out variables\n",
    "* Foil rule\n",
    "* Binomial squares\n",
    "* Difference of squares\n",
    "* Bayes' rule\n",
    "* Expectation properties\n",
    "* Introducing new variables to change expression\n",
    "* Gaussian notation - variance vs standard deviation\n",
    "* Gaussian shift and scale\n",
    "* Multivariate gaussian with diagonal covariance matrix equivalent to independent random variables\n",
    "* Conditional probability - plague\n",
    "* Proportionality\n",
    "* Integral properties\n",
    "* Reparametrization\n",
    "* Monte carlo\n",
    "* Relations with noise\n",
    "* Completing the square\n",
    "* Quadratic formula linear algebra\n",
    "* Data manifold\n",
    "\n",
    "**Logarithm laws**\n",
    "\\\n",
    "Log is standard to use when working with probabilities (avoids underflow, is monotonic and makes derivative easier to work with), therefore knowing about some of the log rules is important.\n",
    "\n",
    "$$\\log \\dfrac{a}{b} = \\log a - \\log b$$\n",
    "$$\\log(a \\cdot b) = \\log a + \\log b$$\n",
    "$$\\log_{e}e^a = \\ln e^a = a$$\n",
    "\n",
    "The rules are the same for the natural log called ln. Also, another log trick that is occasionally used to swap the order of the numerator with the denominator is\n",
    "\n",
    "$$\\log \\dfrac{p}{q} = \\log p - \\log q = - (\\log q - \\log p) = - \\log \\dfrac{q}{p}$$\n",
    "\n",
    "This happens sometimes in KL-divergence in the derivations. For instance, we can switch the numerator and the denominator like so\n",
    "\n",
    "$$D_{KL}(p||q) = E_p\\left[\\log \\dfrac{p}{q}\\right] = \\int_{-\\infty}^{\\infty} p(x) \\log \\dfrac{p(x)}{q(x)} \\, dx = -\\int_{-\\infty}^{\\infty} p(x) \\log \\dfrac{q(x)}{p(x)} \\, dx$$\n",
    "\n",
    "or in expectation\n",
    "\n",
    "$$E_{q(x)}\\left[\\log \\dfrac{p(x)}{q(x)}\\right] = -E_{q(x)}\\left[\\log \\dfrac{q(x)}{p(x)}\\right] = - D_{KL}(q||p)$$\n",
    "\n",
    "where in the last expression we have converted the expectation into a KL-divergence thanks to switching the numerator and the denominator.\n",
    "\n",
    "**Very random division fact**\n",
    "\\\n",
    "This comes up often in the diffusion a unified perspective 2022 paper, mostly because they want to be very explicit with every step, but you wouldn't usually see this in other papers because it would just be simplified.\n",
    "\n",
    "$$\\dfrac{a}{\\left(\\dfrac{b}{c}\\right)} = \\dfrac{ac}{b}$$\n",
    "\n",
    "**Factoring out variables**\n",
    "\\\n",
    "Very easy and common technique is to move variables outside an expression\n",
    "\n",
    "$$\\textcolor{green}{a}x + \\textcolor{green}{a}b = \\textcolor{green}{a}(x + b)$$\n",
    "\n",
    "And this can manifest in so many different ways, but the idea is the same, which is if we can spot a variable occurring in all the terms in an expression, then we can for sure factor it out. Furthermore if we have multiple variables that occur in all terms then we can also factor them out, so it generalizes into something like this\n",
    "\n",
    "$$\\textcolor{green}{ac}x + \\textcolor{green}{ac}b = \\textcolor{green}{ac}(x + b)$$\n",
    "\n",
    "If it doesn't occur in all the terms in the expression but in a handful, we can still choose to factor the variables out, but only from the handful of terms, if it proves to be useful later on. A more complicated example is\n",
    "\n",
    "$$\\textcolor{green}{a}c + \\textcolor{green}{a}d + \\textcolor{blue}{b}c + \\textcolor{blue}{b}d = \\textcolor{green}{a}\\textcolor{red}{(c + d)} + \\textcolor{blue}{b}\\textcolor{red}{(c + d)} = (\\textcolor{green}{a} + \\textcolor{blue}{b})(\\textcolor{red}{c + d})$$\n",
    "\n",
    "**FOIL rule - First outer inner last**\n",
    "\\\n",
    "Apparently this has a name called [FOIL rule](https://en.wikipedia.org/wiki/FOIL_method) and is a mnemonic for describing the process of multiplying two binomials. It's best visualized taken from wiki\n",
    "\n",
    "![](assets/foil.png)\n",
    "\n",
    "The way I think of it is to just multiply every term with the other terms in the parenthesis.\n",
    "\n",
    "**Binomial squares**\n",
    "\\\n",
    "Also common and something everyone has seen in high school\n",
    "\n",
    "$$(x+y)^2 = x^2+2xy+y^2$$\n",
    "$$(x-y)^2 = x^2-2xy+y^2$$\n",
    "\n",
    "Notice that they only differ in the middle term by the sign. In order to derive the form just use the FOIL rule, but if you instead remember this exact form it will save you some time instead of having to add together the middle terms.\n",
    "A more generalized form of this is called the [binomial theorem](https://en.wikipedia.org/wiki/Binomial_theorem#Statement) and is a general formula to compute the exact form up to a power of n (if n isn't too high you can use [Pascals triangle](https://en.wikipedia.org/wiki/Pascal%27s_triangle) to compute the coefficients for each term). But you rarely need more than power of 2. Furthermore, applying this to vectors instead looks almost the same, except we need to transpose one of the them to make it work (adhering to matrix multiplication convention, so shapes must be valid). Let u and v be vectors in $\\mathbb{R}^d$ then we have\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "||u-v||^2 \n",
    "&= (u - v)^T(u-v) \\\\\n",
    "&= \\left\\{\\text{Use FOIL rule}\\right\\} \\\\\n",
    "&= u^Tu - u^Tv - v^Tu + v^Tv \\\\\n",
    "&= \\left\\{\\text{Use } u^Tv = v^Tu = \\sum_{i=1}^d u_iv_i\\right\\} \\\\\n",
    "&= ||u||^2 - 2u^Tv + ||v||^2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Same idea if we had $||u+v||^2$, then the sign in the middle term changes to a plus sign. In the event that u and v are [orthogonal](https://en.wikipedia.org/wiki/Orthogonality) to each other, meaning they are perpendicular, then the middle term vanishes, and for the plus situation we get the [Pythagorean theorem](https://en.wikipedia.org/wiki/Pythagorean_theorem), where the vectors are interpreted as being the [cathetus](https://en.wikipedia.org/wiki/Cathetus) of a perpendicular triangle.\n",
    "Let's use a similar but little bit harder example. Say we had the following shapes u as dxl, a as lx1, v as dxk and b as kx1, then using the formula we get\n",
    "$$||ua-vb||^2 = ||ua||^2 - 2(ua)^T(vb) + ||vb||^2$$\n",
    "\n",
    "Here the idea is even though some terms were matrices u and v, as long as we can get the two terms within the L2-norm to be vectors in the end ua and vb, then it's straightforward to use the formula.\n",
    "\n",
    "**Difference of squares**\n",
    "\\\n",
    "Similar to the above but we have instead for real numbers\n",
    "\n",
    "$$(a-b)(a+b) = a^2 - b^2$$\n",
    "\n",
    "And for vectors u and v\n",
    "\n",
    "$$(u-v)^T(u+v) = u^Tu + u^Tv - v^Tu - v^Tv = ||u||^2 - ||v||^2$$\n",
    "\n",
    "**Bayes' rule**\n",
    "\\\n",
    "In diffusion Bayes' rule is used to flip the condition to compute the distribution for the backward process using the forward process.\n",
    "\n",
    "$$q(x_{t-1}|x_t,x_0) = \\dfrac{q(x_t|x_{t-1}, x_0)q(x_{t-1}|x_0)}{q(x_t|x_0)}$$\n",
    "\n",
    "where $q(x_t|x_{t-1}, x_0)$ is the distribution for the forward process.\n",
    "\n",
    "**Expectation properties**\n",
    "\\\n",
    "The only relevant ones are\n",
    "\n",
    "$$E[X + Y] = E[X] + E[Y]$$\n",
    "$$E[X + k] = E[X] + k$$\n",
    "$$E[aX + k] = aE[X] + k$$\n",
    "$$E[aX + k + bY + l] = aE[X] + k + bE[Y] + l$$\n",
    "\n",
    "where X and Y are random variables and a,k and l are constants. These are important because at times the derivations pushes the expectations out or into expressions. Furthermore, since the formula \n",
    "\n",
    "$$E[X] = \\int_{-\\infty}^{\\infty}xp(x)\\, dx$$\n",
    "\n",
    "expects a distribution this means that when we are working with multiple variables usually we have to clarify in respect to what we are taking the expectation over otherwise it's ambiguous. For instance if we have\n",
    "\n",
    "$$E[X,Y]$$\n",
    "\n",
    "Then should we take the expectation over X or Y or both? It's not clear. However, we can partially solve this by specifying the variables we are interested in as a subscript to the expectation. There are different scenarios in this case that will in general give different results. If $x \\sim p(x)$, $y \\sim q(y)$ and $x,y \\sim r(x,y)$ then we have\n",
    "\n",
    "$$E_{X}[X,Y] = f(Y=y) = \\int_{-\\infty}^{\\infty}xyp(x)\\, dx = y\\int_{-\\infty}^{\\infty}xp(x)\\, dx = yE[X]$$\n",
    "$$E_{Y}[X,Y] = f(X=x) = \\int_{-\\infty}^{\\infty}xyq(y)\\, dy = x\\int_{-\\infty}^{\\infty}yq(y)\\, dy = xE[Y]$$\n",
    "$$E_{X,Y}[X,Y] = \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}xyr(x,y)\\, dxdy$$\n",
    "\n",
    "However, according to [Alecos Papadopoulos](https://stats.stackexchange.com/a/72614) taking w.r.t to a random variable could also mean taking the conditional, which gives the conditional expectation like so for instance\n",
    "\n",
    "$$E_{X}[X,Y] = f(X=x) = \\int_{-\\infty}^{\\infty}xyp(y|x)\\, dy = x\\int_{-\\infty}^{\\infty}yp(y|x)\\, dy = xE[Y|X]$$\n",
    "\n",
    "So it can be confusing what is intended. However, I think the common interpretations are\n",
    "* The conditional interpretation is **rare**, and it's **more common to denote the marginal** instead. If you want a conditional then you usually define it explicitly.\n",
    "* When not specifying what variables to take expectation over at all, then mostly our distribution can be assumed to be a *joint* over all the variables\n",
    "\n",
    "There are also different styles for the subscript, some may write it as for instance\n",
    "\n",
    "$$E_{X \\sim \\mathcal{N}(\\mu, \\sigma)}$$\n",
    "\n",
    "or \n",
    "\n",
    "$$E_{X \\sim p(x)}$$\n",
    "\n",
    "or with only the distribution (which you will often see in generative AI contexts)\n",
    "\n",
    "$$E_{\\textcolor{green}{q_{\\phi}(z | x^{i})}}[\\log p_{\\theta}(x^{i}|z)] = \\int_{\\mathbb{R}^d}\\textcolor{green}{q_{\\phi}(z|x^{i})} \\log p_{\\theta}(x^{i}|z) \\, dz$$\n",
    "\n",
    "to denote a specific distribution that X comes from. Some may even denote a little x (sample from X), instead of X. So it can vary alot. The last expression says take the expectation w.r.t to $q_{\\phi}(z | x^{i})$, and since z is a random variable $f(x^{i}, z, \\theta) = \\log p_{\\theta}(x^{i}|z)$ is a function where the randomness is induced by the random variable z. In this case the distribution for $\\log p_{\\theta}(x^{i}|z)$ is $q_{\\phi}(z | x^{i})$ and therefore applying the formula for expectation is the reason we get the above expression. The interpretation of this integral is that we are \"averaging\" out the randomness of the latent random variable z.\n",
    "\n",
    "Regardless, the point is that the subscript in expectations serves to clarify what variables we are working with and or what distributions we are working with for those variables and usually should be clear from the context. Although, if the common interpretations don't seem to work when using the formulas, try to fall back on the alternative interpretations and if that still doesn't work then it's the fault of the author for not clarifying. At that point you will have no choice but to do some detective work and figure it out by yourself or ask the author if possible.\n",
    "\n",
    "**Introducing new variables to change expression**\n",
    "\\\n",
    "This is a very general mathematical trick that one uses only if they know beforehand what kind of final expression they desire. In ELBO for instance there are some tricks used by papers where they introduce a new variable in a way that doesn't affect the value of the original function/expression, but in doing so can now change the appearance of the expression to something that is closer to what they desire. For instance if we have a log marginal $\\log p(x)$ then we can introduce a new term by multiplying it with 1\n",
    "\n",
    "$$\\log p(x) = \\log p(x)\\underbrace{\\int q_{\\phi}(z|x) \\, dz}_{1}$$\n",
    "\n",
    "where $q_{\\phi}(z|x)$ is a valid probability density function, so it must sum up to 1.\n",
    "Another example is multiplying the existing expression with a new term and then dividing with the new term. Because we introduce it and cancel it out then it's as if the original expression hasn't undergone any changes at all. Let our original expression be\n",
    "\n",
    "$$E_{q_{\\phi}(z|x)}\\left[\\log \\dfrac{p(x,z)}{p(z|x)}\\right]$$\n",
    "\n",
    "And let's introduce $q_{\\phi}(z|x)$ as an example\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "E_{q_{\\phi}(z|x)}\\left[\\log \\dfrac{p(x,z)\\textcolor{green}{q_{\\phi}(z|x)}}{p(z|x)\\textcolor{green}{q_{\\phi}(z|x)}}\\right] \n",
    "&= E_{q_{\\phi}(z|x)}\\left[\\log \\dfrac{p(x,z)}{\\textcolor{green}{q_{\\phi}(z|x)}}\\right] +  E_{q_{\\phi}(z|x)}\\left[\\log \\dfrac{\\textcolor{green}{q_{\\phi}(z|x)}}{p(z|x)}\\right] \\\\\n",
    "&= E_{q_{\\phi}(z|x)}\\left[\\log \\dfrac{p(x,z)}{q_{\\phi}(z|x)}\\right] + D_{KL}(q_{\\phi}(z|x) || p(z|x))\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "So we can see how we started from a seemingly arbitrary expected value and by introducing a new term changed the appearance of the original expression into a form we desired, being able to split up the expression into an expectation and a KL-divergence, without changing the original value. These are common tricks people use in ELBO. Note that it doesn't have to be these specific terms, rather the point is that one can introduce new terms, whatever it may be, in order to change the appearance of the expression, so long as it doesn't change the original value.\n",
    "\n",
    "**Gaussian notation - variance vs standard deviation**\n",
    "\\\n",
    "The variance or standard deviation part of the gaussian notation varies from author to author. Some write the variance, some write the standard deviation. An advice to spot which one is which, is usually to pay attention to anything that is set to the power of 2 will most likely be variance, whereas for standard deviation it's either no power meaning to the power of 1 or square root even. Here are some examples\n",
    "\n",
    "* $\\mathcal{N}(0, \\sigma^2)$ meaning variance, where $\\sigma$ usually denotes standard deviation\n",
    "* $\\mathcal{N}(0, \\sigma)$ meaning standard deviation\n",
    "* $\\mathcal{N}(\\sqrt{\\alpha_t}x, \\sqrt{1-\\alpha_t}I)$ meaning $\\sqrt{1-\\alpha_t}I$ is the standard deviation, whereas $(1-\\alpha_t)I$ is the variance for a multivariate gaussian. This is the forward process in diffusion by the way.\n",
    "\n",
    "If you are wondering why there seems to be a relation between variance and standard deviation it's because the relation is\n",
    "\n",
    "$$\\text{standard deviation}^2 = \\text{variance} \\implies \\sqrt{\\text{variance}} = \\text{standard deviation}$$\n",
    "\n",
    "**Gaussian shift and scaling**\n",
    "\\\n",
    "Recall that the formula for shifting and scaling a gaussian distribution is\n",
    "\n",
    "$$Y = aX + b \\sim \\mathcal{N}(a\\mu + b, |a|\\sigma I)$$\n",
    "\n",
    "where X is a random variable that is distributed by a standard gaussian distribution, that is $X \\sim \\mathcal{N}(0, I)$.\n",
    "\\\n",
    "Shift means add with something and scaling means multiply with something.\n",
    "This means that when you are shifting a gaussian distribution by b and the mean is $\\mu$, then the result is $\\mu + b$, while for scaling with a and if the standard deviation is $\\sigma$ the result for the mean is $a\\mu$ and for the standard deviation $|a|\\sigma$. \n",
    "In the forward process a gaussian shift and scale occurs at each timestep. We have this general equation for computing a latent at any time step t\n",
    "\n",
    "$$z_t = \\sqrt{\\alpha_t}x + \\sqrt{1-\\alpha_t}\\epsilon$$\n",
    "\n",
    "where $z_t$ is the latent variable (hidden), $\\alpha_t = \\Pi_{t=1}^{T}1-\\beta_t$, $\\beta_t \\in [0, 1]$ and $\\epsilon \\sim \\mathcal{N}(0, I)$. If we now use the formula for shifting and scaling a gaussian then this can be interpreted as scaling the gaussian noise $\\epsilon$ by $1-\\alpha_t$ on the variance ($\\sqrt{1-\\alpha_t}$ on the standard deviation) and shifting by $\\sqrt{\\alpha_t}x$ on the mean at each timestep t. Note that the scaling didn't affect the mean simply because the mean is zero, so multiplication has no effect on it. \n",
    "\n",
    "To help with visualizing scaling and shifting a gaussian let's look at this example where you shift the gaussian by k steps to the right in the x-axis. That's what happens when you shift by a positive k.\n",
    "\n",
    "![](assets/shifting_gaussian.png)\n",
    "\n",
    "Scaling is not as visually elegant as shifting, since you modify both the mean and the variance, but it's very straightforward to calculate the effect using the formula.\n",
    "\n",
    "**Multivariate gaussian with diagonal covariance matrix equivalent to independent random variables**\n",
    "\\\n",
    "Only for gaussian distribution does zero covariance imply independence. A diagonal covariance matrix means zero covariance, because the off-diagonals are zeros. This means that since we are working with multivariate gaussians in the forward process, where the covariance is a diagonal matrix for the gaussian noise we add to the latent variables, we can therefore interpret each entry in the 2D matrix of the noise $\\epsilon$ as coming from their own univariate gaussian distribution. To see this I will refer to [cs229 from Stanford](https://cs229.stanford.edu/section/gaussians.pdf) section 3.   \n",
    "\n",
    "**Conditional probability - plague**\n",
    "\\\n",
    "Recall that conditional probability of two random variables can be written as\n",
    "\n",
    "$$\\dfrac{p(A,B)}{p(B)} = p(A|B)$$\n",
    "\n",
    "If we condition the marginal $p(B)$ with a random variable C then we get\n",
    "\n",
    "$$\\dfrac{p(A,B|C)}{p(B|C)} = p(A|B,C) \\iff p(A,B|C) = p(A|B,C)p(B|C)$$   \n",
    "\n",
    "So the condition C infects the other expressions like a plague. Looking at the marginal $p(B)$ once conditioned on C $p(B|C)$ it means that if A is now conditioned on B then since B is conditioned on C they have to appear together in the conditional probability of A given B given C. So the conditional $p(A|B)$ changes to $p(A|B,C)$, so we condition on both now. Then using the joint formula when multiplying the two we finally get $p(A,B|C) = p(A|B,C)p(B|C)$. That's how I like to think of it.\n",
    "\n",
    "\n",
    "**Proportionality**\n",
    "\\\n",
    "Sometimes you will see this sign $\\propto$, which just means that something is proportional to something else. $a \\propto b$ means that a is proportional to b meaning $a = k \\cdot b$ for some $k \\in \\mathbb{R}$. This is usually used in context of bayes' rule, where we ignore some parts of an expression, usually the normalizing constant. Therefore, we sometimes see something like $p(a|b) = \\dfrac{p(b|a)p(a)}{p(b)} \\propto p(b|a)p(a)$. The reason for this is the normalizing constant is usually intractable to compute for high dimensional data or complex expressions that don't have a closed-form solution, so instead we can simplify the computation by putting a proportional equality instead.\n",
    "\n",
    "**Integral properties**\n",
    "\\\n",
    "There are alot of properties, just listing the relevant ones\n",
    "* $\\int yf(x) \\, dx = y\\int f(x) \\, dx$. This is because y is not dependent on any x, and since we integrate w.r.t to x then y is considered a constant that we can move outside\n",
    "* $\\int\\int g(y)f(x) \\, dxdy = \\int g(y) \\, dy \\int f(x) \\, dx$\n",
    "* $\\int yp(x) \\, dx = y\\int p(x) \\, dx = y \\cdot 1 = y$. This is if p(x) is a valid probability density function, so that it integrates to 1. This is usually used to marginalize out irrelevant variables that we don't need to consider, because the marginalization of proper distribution should give us an integral of 1, which means they will dissappear without affecting whatever is left.\n",
    "\n",
    "**Reparametrization**\n",
    "\\\n",
    "Reparametrization is a general term for anything where you \"change\" the parameters that a function is dependent on or that an expression contains. This can produce an entirely different expression, but the results should still remain the same, that is the range of the original function prior to being reparametrized has to be the same as after the reparametrization so that they describe the same graph or curve so to speak. The purpose of reparametrization is to put emphasis on different mathematical perspectives or make computation easier, because the new expression can be easier to work with in certain situations. For instance if we look at the famous unit circle equation\n",
    "\n",
    "$$x^2+y^2=1$$\n",
    "\n",
    "This is based on the euclidean pythagorean theorem. While this produces a unit circle it's not very pleasant to work with if we want to work with angles to describe x- and y-coordinates instead. Therefore, we can instead look to trigonometric functions, in particular cos and sin. How do we incorporate these into our original equation? We reparametrize it by substituting them into the original equation. We have these relations that we know from basic algebra\n",
    "\n",
    "$$cos \\theta = x$$\n",
    "$$sin \\theta = y$$\n",
    "\n",
    "where $\\theta \\in [0, 2\\pi]$ is the angle between the line from origo to the perimeter of the circle and the x-axis.\n",
    "So after substitution we get\n",
    "\n",
    "$$cos^2\\theta + sin^2\\theta = 1$$\n",
    "\n",
    "And this is the unit circle once again, but reparametrized with $\\theta$, so that it no longer depends on x and y. Why is this more useful? To name a few benefits, the equation now only depends on one variable so it's easier to describe lines within the circle if we want to extend it to arbitrary length radius r. It's easier to analyze since we don't have to look at the piecewise parts $y = \\pm \\sqrt{1 - x^2}$ anymore. Integration is easier as well as we don't need double integration now that we only have one variable. Alot of phenomenon in nature and physics can be naturally described with trigonometric functions such as harmonic motions, waveforms etc, so it makes it more suitable to work with trigonometric expressions. I will show you later on how the reparametrizations are used in diffusion, but this section is to show the easy idea of what reparametrization is and what one possible reparametrization could look like and why we would want to do it. Below is simple illustration of a unit circle, blue represents the reparametrized function of a unit circle with $\\theta$ and the red is the one with x and y.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae6dd879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the angle range for the parametric circle\n",
    "theta = np.linspace(0, 2 * np.pi, 100)\n",
    "\n",
    "# Parametric formulation of the unit circle\n",
    "x_parametric = np.cos(theta)\n",
    "y_parametric = np.sin(theta)\n",
    "\n",
    "# x^2 + y^2 = 1 formulation\n",
    "x_standard = np.linspace(-1, 1, 400)\n",
    "y_positive = np.sqrt(1 - x_standard**2)\n",
    "y_negative = -np.sqrt(1 - x_standard**2)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "# Plot the parametric unit circle\n",
    "plt.plot(x_parametric, y_parametric, label='Parametric: $x = \\cos(\\\\theta)$, $y = \\sin(\\\\theta)$', color='blue')\n",
    "\n",
    "# Plot the standard unit circle\n",
    "plt.plot(x_standard, y_positive, label='$y = \\sqrt{1 - x^2}$', color='red', linestyle='dashed')\n",
    "plt.plot(x_standard, y_negative, label='$y = -\\sqrt{1 - x^2}$', color='red', linestyle='dashed')\n",
    "\n",
    "# Formatting the plot\n",
    "plt.axhline(0, color='grey', linestyle='--', linewidth=0.5)\n",
    "plt.axvline(0, color='grey', linestyle='--', linewidth=0.5)\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.title('Comparison of Unit Circle Formulations')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c379ef",
   "metadata": {},
   "source": [
    "**Monte carlo**\n",
    "\\\n",
    "The point of Monte Carlo in diffusion usually is to approximate the integral of something that can't be solved analytically or is intractable. The integral arises because of expectation of random variables with continuous values as it's domain. Specifically with Monte Carlo we can approximate the expectation with the sample mean\n",
    "\n",
    "$$E_{x \\sim p}[f(x)] = \\int_x f(x) p(x) \\, dx \\approx \\dfrac{1}{N}\\sum_{n=1}^{N}f(x^{n})$$\n",
    "\n",
    "One can show that the MC is an unbiased estimator and if the variance is finite then it will decrease monotonically as the number of samples N increases. Specifically in diffusion, for the noise formulation, in theory MC is used to approximate the reconstruction term that is the term for going from time $t=1$ to $t=0$ which can be described as $q(x|z_1)$, much like how it's used in VAE. However, [Luo](https://arxiv.org/pdf/2208.11970) seems to disregard the term entirely, while [Prince chapter 18](https://udlbook.github.io/udlbook/) shows how this term can be merged into the backward posterior (the main term we want the model to learn) in equation 18.38 page 362, so we don't need to bother with MC, because we just use the formula for [KL divergence between two multivariate gaussians](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Multivariate_normal_distributions) on the main term, so we can compute it in closed-form. If this is unclear look at equation 58 and 86 in the [Luo](https://arxiv.org/pdf/2208.11970) paper, specifically the denoising matching term in 58. It can be written as\n",
    "\n",
    "$$\\sum_{t=2}^T E_{q(x_1|x_0)}\\left[D_{KL}(q(x_{t-1}|x_t,x_0)||p_{\\theta}(x_{t-1}|x_t))\\right]$$\n",
    "\n",
    " So in the end we don't directly use any MC on the relevant terms, but some form of MC is still carried out I think. The timesteps almost seems MC because of the randomness introduced, but since we are not summing over anything and weighting it for the timesteps, then I don't think it is MC. Furthermore, one could argue that the mini-batch approach in mini-batch GD, where you randomly sample a batch from the training, can be seen as using MC, because the loss function over the training data can be written as an expectation and then using MC to approximate this expectation would result in the mini-batch approach. But this would then be true for all neural networks that use mini-batch GD to train it. However, as far as using MC on the original expectation over the denoising matching term, this is not done as far as I'm concerned.\n",
    "\n",
    "**Specific relations with noise**\n",
    "\\\n",
    "Remember that the general noise formula at time t is \n",
    "\n",
    "$$\\alpha_t = \\Pi_{t=1}^T 1-\\beta_t = (1 - \\beta_1)...(1 - \\beta_t)$$\n",
    "\n",
    "where $\\beta_t \\in [0,1]$. Based on this formula we therefore have useful relations such as these\n",
    "\n",
    "$$(1 - \\beta_t) * \\alpha_{t-1} = (1-\\beta_1)...(1-\\beta_{t-1}) \\cdot (1-\\beta_t) = \\alpha_t$$\n",
    "$$\\dfrac{\\alpha_t}{1-\\beta_t} = \\alpha_{t-1}$$\n",
    "\n",
    "Both are used in Prince chapter 18 and the 2022 paper diffusion a unified perspective.\n",
    "\n",
    "**Completing the square**\n",
    "\\\n",
    "[Wikipedia](https://en.wikipedia.org/wiki/Completing_the_square) has another form for completing the square. I just think of it in simpler terms. We have an expression with one quadratic term and the rest of the terms are either something that looks like a cross-term and or constants. What we would like to achieve is to put them into one quadratic term and whatever is remaining is put into some constant c.\n",
    "\n",
    "$$x^2 + 2xy + k = (x + y)^2 - y^2 + k = (x+y)^2 + c$$\n",
    "\n",
    "where $c = -y^2 + k$. This is usually used to \"pattern match\" an expression that we are interested in. In diffusion it's mostly for the quadratic term in gaussian distribution $-\\dfrac{1}{2}\\dfrac{(x - \\mu)^2}{\\sigma^2}$, but we are only interested in the numerator. Let's say we have an expression like this\n",
    "\n",
    "$$-\\dfrac{1}{2\\sigma^2}\\left[\\textcolor{green}{x^2 - 2xy}\\right]$$\n",
    "\n",
    "then trying to pattern match the green part within the square brackets with the quadratic term (because they look almost identical) in a gaussian distribution gives us\n",
    "\n",
    "$$\\left[x^2 - 2xy\\right] = \\left[x - y\\right]^2 - y^2 \\stackrel{+}{=} \\left[x - \\underbrace{y}_{\\mu}\\right]^2$$\n",
    "\n",
    "where we ignore the rest term $y^2$. The $\\stackrel{+}{=}$ is an additive equality, which means they are equal up to an additive constant, so they are only different by some constant you add together. In this case the only difference between the two was $-y^2$, but by using this notation we can therefore ignore the $-y^2$, and therefore perfectly pattern match with the quadratic term of the gaussian distribution. Obviously one has to note that once we go down this route of using additive equality or proportional equality, then we aren't computing the same thing anymore, and therefore only working with an **approximation**. We can see from this that the $\\mu$, which is the mean of the gaussian, constitutes whatever $y$ is. This is a very simple example of how to pattern match things in math and completing the square is one tool to achieve this when we are working with quadratic terms.\n",
    "\n",
    "**Quadratic form with diagonal matrix - linear algebra**\n",
    "\\\n",
    "This is a special case of the [quadratic form](https://en.wikipedia.org/wiki/Quadratic_form#Associated_symmetric_matrix) you would usually encounter in linear algebra\n",
    "\n",
    "$$x^T M^{-1} x$$\n",
    "\n",
    "where x is a column vector 1xD, $x^T$ is a row vector Dx1 and M a DxD diagonal matrix. Remember that the inverse of a diagonal matrix is just the reciprocal of each diagonal elements. What's so special with diagonal matrix is that the off-diagonals are zero and if the variances at the diagonals are the same, then we can see that we get\n",
    "\n",
    "$$\\dfrac{1}{\\sigma^2}||x||_2^2$$\n",
    "\n",
    "I'm not going to provide the proof, but it's very simple and I encourage the curious reader to try it themselves, the key lies in that you are doing matrix multiplications with a diagonal matrix, so the off-diagonals will cancel out alot of possible entries in the result, and the fact that the diagonals are assumed to be the same means we can factor out the value. The use of this in diffusion is when you apply the formula for [KL-divergence between two gaussian distributions](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Multivariate_normal_distributions), where we assume the covariance matrix of each gaussian to be diagonal and the variances are the same for their respective diagonals.\n",
    "\n",
    "**Data manifold**\n",
    "\\\n",
    "I don't quite understand this myself, because a technical definition of manifolds require knowledge in [topology](https://en.wikipedia.org/wiki/Topology), which is advanced math. I will however add my understanding of it here, because it seems to be mentioned from time to time, without any explanations. [Wiki](https://en.wikipedia.org/wiki/Manifold) says the following about manifold\n",
    "> In mathematics, a manifold is a [topological space](https://en.wikipedia.org/wiki/Topological_space) that **locally resembles Euclidean space** near each point. \n",
    "\n",
    "For instance, the planet earth is a good example of a manifold, for us humans when we are walking on the surface of the planet it comes across as being flat (we are locally moving along the surface of the manifold), but the planet itself is really just a sphere. In a sense it's a 2D manifold embedded into a 3D space, because we can think of charting these flat surfaces of the planet and putting them together, it would be almost like unrolling the sphere into a 2D map. But in doing so, we have now transformed the 3D sphere into a set of 2D charts. In machine learning context this means that we could perhaps describe this 3D sphere with less variables than originally presented, effectively reducing the dimensionality. And this idea that most high-dimensional data lies on a lower dimensional manifold is called [manifold hypothesis](https://en.wikipedia.org/wiki/Manifold_hypothesis) and is the reason why we try to perform dimensionality reduction on high-dimensional data to begin with. In the case of diffusion we are just saying that the image dataset lies on a lower-dimensional manifold that we are trying to learn to represent with our model.\n",
    "\n",
    "Just for illustration of unrolling a manifold, let's consider a swiss roll like the image below in 3D space. Clearly this seems to have some 2D structure. If we unroll this structure we get something in 2D space instead, which shows that we could describe this 3D object as a lower-dimensional manifold with fewer variables in 2D.\n",
    "\n",
    "![](./assets/swiss_roll_manifold.png)\n",
    "![](./assets/unrolling_swiss_roll_manifold.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808c19de25b17ad7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Diffusion informal walkthrough - noise formulation\n",
    "This section focuses more on the intuition rather than the rigorous parts of diffusion. The purpose is to tie together insights that we have collected as a group about diffusion into a coherent walkthrough.\n",
    "\n",
    "**Outline**\n",
    "* Intro\n",
    "* Assumptions, data format and preprocessing\n",
    "* Forward process\n",
    "* Loss function\n",
    "* Training\n",
    "* Backward process\n",
    "\n",
    "### Intro\n",
    "Imagine we have an image dataset and we want to train a model to generate samples that are representative of the data images. How do we do it? There are lots of different methods nowadays, and they come with different flavours. I'm not going to cover any of the other methods, as I quite frankly don't remember them well enough to even provide a brief description, but I will include two images that shows a nice comparison of diffusion models and some of the other methods that are popular.\n",
    "\n",
    "![](assets/advantage_genai_models.png)\n",
    "\n",
    "This shows that diffusion models cover two of the features, high quality samples and mode coverage/diversity, while being slower to sample compared to GANs, VAEs and models using normalizing flows. The slower sampling has to do with the backward process in DDPM being markovian, thus not able to jump time steps and needing a large number of steps to converge to a clean image (hundreds to thousand for DDPM). The [DDIM](https://arxiv.org/pdf/2010.02502) paper however seems to speed this up by instead working with a non-markovian structure for the backward process, thus being able to jump time steps. There are other papers since DDPM and DDIM that has tried to improve the speed of sampling ($\\textcolor{red}{\\text{Insert popular references}}$). We also have this nice table to show general comparisons between them.\n",
    "\n",
    "![](assets/genai_models_table_features.png)\n",
    "\n",
    "Here likelihood basically means whether computing the likelihood p(x) for any given **data point** x is exact or not. Diffusion models only do this approximately. Also reconstruction is hard for a diffusion model since the diffusion paths vary alot even from the same starting point, so it's never guaranteed that you would end up with the exact same image starting from the exact same noise. As for the bottleneck the dimension is the same for input and output, so it never changes throughout the process, unlike VAE that has a bottleneck between the encoder and decoder after the dimensionality has been reduced. The mode covering/diversity feature of diffusion models is thanks to a multitude of noise sources (including the scheduler) and stochasticity being introduced during the forward and backward process. This has a positive effect on preventing mode collapse, since areas with low probability density in the data space will be assigned more probability or be explored more in the diffusion paths.  Also the training objective is a minimization of a KL-divergence that is a M-projection (forward KL-divergence) $D_{KL}(\\text{true distribution} || \\text{approximate distribution})$, meaning it's mode covering to begin with. \n",
    "\n",
    "One thing to note about diffusion models is that while it takes inspiration from bayesian approaches, since it's a markovian hierarchical VAE, and VAEs are bayesian (they use a form of variational inference), it doesn't **directly** use any bayesian, MLE or MAP. It's not MLE because we aren't optimizing directly on the likelihood $P(D|\\theta)$. It's not MAP, because we aren't using any priors directly in the optimization. The standard gaussian at the end of T can be seen as some kind of prior in the forward process, but it's not being used as a weighting on all terms in the optimization as you would in MAP $\\operatorname*{argmax}_{\\theta} P(D|\\theta)\\textcolor{green}{P(\\theta)}$. It's not VI because we aren't computing $P(z|x)$ (we have the diffusion kernel for that which is not learnt), where z is a latent and x is the data, which is what VI and the encoder in VAE does. Instead, and as you will see later on in the loss section, we are trying to compute $q_{\\theta}(x_{t-1}|x_t)$ that is the posterior in the backward process and the form can be derived by using Bayes' rule.\n",
    "\n",
    "Anyways, since this notebook is about diffusion the following sections will give intuition on how diffusion in DDPM works. The brief overview is that even for diffusion models there are different formulations one can go with that leads to different loss functions for diffusion and in this section we go with the noise formulation, where the model has to learn the noise in a noisy image. This formulation seems to be the most popular on the internet when covering the basics of diffusion, mostly because the [original stable diffusion](https://arxiv.org/pdf/2112.10752), [GLIDE](https://arxiv.org/pdf/2112.10741) and [DALL-E 2](https://arxiv.org/pdf/2204.06125v1) used it.\n",
    "\n",
    "\n",
    "### Assumptions, data format and preprocessing\n",
    "Assumptions\n",
    "* Data is i.i.d, meaning each sample comes from the same distribution and are independent. We call this distribution the data distribution $p(x)$.\n",
    "* Forward and backward process follows a markov chain, i.e next latent is only dependent on previous latent\n",
    "* Latent dimension and data dimension are equal throughout the forward and backward process\n",
    "* Distribution of intermediate latents in the forward are gaussians, these gaussians are not learnt. This means the encoder part of diffusion is never learnt, it's pre-defined as gaussians together with a scheduler. \n",
    "* Distribution of latent at the end of forward process converges to standard gaussian when timesteps $t \\to \\infty$\n",
    "\n",
    "Data format is 2D images but with color channel (greyscale only has 1 color channel) becomes 3D. With a batch this can even go to 4D. But in the formulas we are usually only thinking of a single image at a time, and that image is percieved as 2D I think, even though in practise it's 3D because of color channels. These are not flattened as in VAE, where we would have a 1D array. The preprocessing step in DDPM seems to be to transform the image data from discrete pixel values $\\{0,.., 255\\}$ to continuous values $[-1,1]$. Note that as a natural consequence of how we define the forward process, the backward process will also be gaussian. Now whether you learn this distribution will depend on what formulation you choose, we will cover this in the loss section.\n",
    "\n",
    "### Forward process\n",
    "**Idea of forward process**\n",
    "\\\n",
    "The idea of the forward process is usually described as starting with a clean image and destroying it through a series of noisifying steps, where gaussian noise is added to it. Note that for the entire forward process we are only working with continuous values for the data and the noise. How much noise is added at each time step is decided by the scheduler that varies as a variable $\\beta \\in [0,1]$. It starts off from 0 and increases gradually to 1. The purpose is to eventually reach a state where the image becomes random noise distributed by a standard gaussian $N(0,I)$. For the DDPM they used a linear scheduler starting from $\\beta_1 = 10^{-4}$ to $\\beta_T = 0.02$. We call each intermediate noisy image a latent. Mathehmatically this process is characterized as a Markov chain, where each latent is only dependent on the previous latent. Illustration of this\n",
    "\n",
    "![](assets/forward_process.png)\n",
    "\n",
    "As can be seen each latent is generated only depending on the previous latent. This is the markovian structure for diffusion. And as time goes, the original image gets more noisy.\n",
    "\n",
    "**Diffusion kernel - jump steps**\n",
    "\\\n",
    "However, to make it more efficient, one can bypass this restriction of having to go one step at a time, by using the diffusion kernel to immediately jump to the latent at timestep t, which makes it more efficient to sample latents. The diffusion kernel is an important part of the diffusion process and can be described as \n",
    "\n",
    "$$x_t = \\sqrt{\\alpha_t}x_0 + \\sqrt{1 - \\alpha_t}\\epsilon$$\n",
    "\n",
    "where $x_t$ is the latent variable (hidden), $\\alpha_t = \\Pi_{t=1}^{T}1-\\beta_t$, $\\beta_t \\in [0, 1]$ and $\\epsilon \\sim \\mathcal{N}(0, I)$. In general $\\alpha_t \\to 0$, $1-\\alpha_t \\to 1$ and $\\beta_t \\to 1$ as $t \\to T$, where $\\beta_t$ starts from 0. Alternatively one can describe it as a conditional gaussian distribution\n",
    "\n",
    "$$x_t \\sim q(x_t | x_0) = \\mathcal{N}(\\sqrt{1 - \\alpha_t}x_0, \\sqrt{1 - \\alpha_t}I)$$\n",
    "\n",
    "There is a dual interpretation of diffusion kernel as far as I'm concerned \n",
    "1. It's a gaussian shift and scaling, since we are adding $\\sqrt{\\alpha_t}x_0$ to the noise that comes from a standard gaussian distribution and scaling it by $\\sqrt{1 - \\alpha_t}$. This also means that the starting image $x_0$ heavily affects the possible positions for the latent $x_t$, which is natural, because the clean image is used as a starting point to add noise to.\n",
    "2. Scaling the image down and adding noise to it, this is probably the most common interpretation on the internet, at least in introductory blogs/videos about diffusion.\n",
    "\n",
    "Note that the diffusion kernel is not learned, it is completly pre-determined given an image $x_0$ and a scheduler. \n",
    "Furthermore, if one views the $x_0$ as a random variable distributed by the data distribution, then we can interpret it together with the forward process as gradually transforming a very complex data distribution into a gaussian distribution. Another way of looking at it is to say that the data is transformed from in-distribution to out-distribution, where the in-distribution is the data distribution of the images and out-distribution is anything outside the data distribution, basically meaning that the noisy image is no longer in the data manifold.\n",
    "\n",
    "![](assets/in_and_out_dist.png)\n",
    "\n",
    "If we assume the circle to be the data distribution, then adding noise to an image will bring it outside.\n",
    "\n",
    "**Why scale the image down?**\n",
    "\\\n",
    "One might wonder where the scalars $\\sqrt{\\alpha_t}$ and $\\sqrt{1-\\alpha_t}$ come from in the diffusion kernel. $\\sqrt{1-\\alpha_t}$ is for scaling up the variance and [Chan](https://arxiv.org/pdf/2403.18103) shows that when $t \\to \\infty$ then in the limit in order for the variance of the gaussian distribution to be I the scalar has to be precisely $\\sqrt{1-\\alpha_t}$, but the $\\sqrt{\\alpha_t}$ is not explained at all and comes out of nowhere. From what I've seen it should have a variance preserving purpose, to not make the variance explode, but I don't understand it myself. The simplest explanation though is that we want the mean to converge to zero at the end of the forward process, so that's why we need to scale the image down in the diffusion kernel. However, this doesn't still explain why specifically it has to be $\\sqrt{\\alpha_t}$, because it could have been any scalar that can scale down the image. This remains a mystery for now. Ideally a proof would be nice to see.\n",
    "\n",
    "**Shape of noise**\n",
    "\\\n",
    "The shape of the noise is the same as the data, which is 3D. Because we are actually working with multivariate gaussians and the noise is sampled from a standard gaussian with a diagonal covariance matrix, this means that each entry in the 3D shaped noise array can be seen as being sampled from their own univariate gaussian distributions. And after scaling this noise and adding it to the clean image, we can interpret that as we are adding each pixel of the image with noise, where each noise element in the 3D array comes from their own univariate gaussian distribution. Once again, the intuition for why zero covariance (diagonal covariance matrix) implies zero independence only for gaussians can be found in the [cs229 stanford](https://cs229.stanford.edu/notes-spring2019/cs229-notes2.pdf) section 3.\n",
    "\n",
    "**Why does forward process work?**\n",
    "\\\n",
    "Intuitively it's a much harder task to go from pure noise to a clean image in one step. Going one step at a time with smaller adjustments to the clean image by adding noise to it, will only change the distribution slightly, whether it's marginal $p(x_t)$ or conditional $p(z_t|z_{t-1})$. The change is much easier to learn than the change from pure noise to data distribution, because each intermediate distribution is similar to the previous one.\n",
    "\n",
    "**Marginal vs conditional**\n",
    "\n",
    "> I'm not sure about this particular subsection. Needs to be revised!!!\n",
    "\n",
    "Earlier we had some discussions on the difference between the marginal and the conditional of the latents. The marginal was brought up, because [Prince chapter 18](https://udlbook.github.io/udlbook/) had visualized the marginals in the forward process. We came to the conclusion that the marginal $q(x_t)$ can be seen as representing the average bluriness in the image at time t, while the conditional $q(z_t|x)$ that is the diffusion kernel, can be seen as the bluriness for a given image x. For the marginal case I think this can be made clearer by observing the following expression\n",
    "\n",
    "$$E_{p(x)}\\left[q(x_t|x)\\right] = \\int q(x_t|x)p(x) \\, dx = \\int q(x_t,x) \\, dx = q(x_t)$$\n",
    "\n",
    "where the integral is taken over all possible x, meaning all possible data points. This basically says to compute the average of the latent $x_t$ conditioned on x where x comes from the data distribution. So it's the average bluriness in the image at time t.\n",
    "However, note that this marginal is different from the marginal likelihood that is the data distribution. This only concerns with the latents and this can be seen if we just look at how it's computed\n",
    "\n",
    "$$q(x_t) = \\int_x q(x_t|x)p(x) \\, dx$$\n",
    "\n",
    "where $q(x_t | x)$ is the diffusion kernel and $p(x)$ is the data distribution, whereas for the marginal data distribution we have\n",
    "\n",
    "$$p(x) = \\int q(x_T) \\Pi_{t=1}^T q_{\\theta}(x_{t-1} | x_t) \\, dx_{1:T} = \\int q(x_{0:T}) \\, dx_{1:T}$$\n",
    "\n",
    "so it should be clear that they are not the same? \n",
    "\n",
    "**Forward process in practise**\n",
    "\\\n",
    "While many sources on the web make diffusion out to be this markovian chain, where one step is taken at a time to generate the next latent. In practise, the only time the forward process comes into play is during training, but even then it's different. Time steps are uniformly sampled, so we never sequentially iterate from 1 to T. Why sample time steps? No idea, heard it has a regularizing effect and perhaps it relates to Monte carlo, but not sure.\n",
    "\n",
    "**Latent diffusion model**\n",
    "\\\n",
    "Lastly, since this has been brought up in the group multiple times, the biggest difference between latent diffusion models such as stable diffusion and DDPM is that there's a VAE before training to downsample the image latent to lower dimensions, which makes it faster to train and infer. After sampling the latents are upsampled to the original image resolution.\n",
    "\n",
    "### Loss function - noise and other formulations\n",
    "\\\n",
    "Deriving the loss function is the hardest part of the theoretical side of diffusion, at least DDPM diffusion. There's alot of algebraic manipulations and mathematical tricks applied. I won't derive anything here, read the suggested papers in the next section if you want to see full derivations. Let's start with the log likelihood data $p(x)$. Because integrating out all the possible latents is intractable we can instead with clever manipulations arrive at the ELBO. Our job is to maximize this ELBO, so we can approach the data log likelihood $\\log p(x)$, we want it as close as possible, because if we can train a model to approximate it well, then samples drawn from it will be representative of the data distribution.\n",
    "\n",
    "$$\\log p(x) \\geq \\underbrace{\\underbrace{E_{q(x_1|x_0)}\\left[\\log p_{\\theta}(x_0|x_1)\\right]}_{\\text{reconstruction term}} - \\underbrace{D_{KL}(q(x_T | x_0) || p(x_T))}_{\\text{prior matching term}} - \\underbrace{\\sum_{t=2}^T D_KL(q(x_{t-1}|x_t,x_0)  || p_theta(x_{t-1} | x_t))}_{\\text{denoising matching term}}}_{\\text{ELBO}}$$\n",
    "\n",
    "where the reconstruction term is the same as in VAE and can be approximated with Monte carlo. However, later on this term can be ignored because it's very similar to $x_0$ if you can make the scheduler have a small gap between timestep 0 and 1, but an alternative explanation that I like better is given by Prince, where he shows that you can merge this term together with the denoising matching term. The prior matching term is zero under the assumption that our prior is the standard gaussian, so we can get an exact match. As for the denoising matching term this needs to be massaged a bit more before one can turn it into a loss function. Clever use of Bayes' rule on the backward distribution and [KL-divergence for two multivariate gaussians](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Multivariate_normal_distributions) gives the posterior for the backward process distribution\n",
    "\n",
    "$$q(x_{t-1}|x_t) \\sim \\mathcal{N}\\left(\\underbrace{\\dfrac{\\sqrt{\\alpha_t}(1-\\alpha_{t-1})x_t + \\sqrt{\\alpha_{t-1}}(1-\\alpha_t)\\textcolor{green}{x_0}}{1-\\alpha_t}}_{\\mu_q(x_t, x_0)}, \\underbrace{\\dfrac{(1-\\alpha_t)(1-\\alpha_{t-1})}{1-\\alpha_t}I}_{\\sum_q(t)}\\right)$$\n",
    "\n",
    "Now the biggest take-away from [Luo 2022](https://arxiv.org/pdf/2208.11970) is that at this point reparametrizing $\\textcolor{green}{x_0}$ in the mean of this posterior backward distribution will give different objectives/formulations of diffusion, basically resulting in different loss functions for diffusion. To get the noise formulation we need to isolate $x_0$ in the diffusion kernel and substitute the expression into the mean. Simplifying the resulting expression and substituting that into the denoising matching term and applying [KL-divergence for two multivariate gaussians](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Multivariate_normal_distributions) gives the final squared l2 distance of the noise differences giving the following objective \n",
    "\n",
    "$$\\operatorname*{argmin}_{\\theta} ||\\epsilon_0 - \\epsilon_{\\theta}(x_t, t)||_2^2$$\n",
    "\n",
    "There is some scaling factor that comes with it as well, but it's ignored to reweight the ELBO. This in combination with the training setup in DDPM leads to down-weighting the earlier timesteps where the noise is small between the steps to make the model focus on harder denoising tasks in later timesteps. In addition to the noise formulation one can reparametrize the mean into 3 additional objectives, which all differ up to a scaling factor.\n",
    "1. Predict the original image given noisy image at time t\n",
    "2. Predict the mean of the backward posterior distribution $q(x_{x-1}|x_t)$\n",
    "3. Predict the score function\n",
    "\n",
    "It's interesting to note that for the noise formulation we are using the mean of the backward posterior distribution as a **proxy** to learn the noise by reparametrizing the mean with $\\epsilon_0$ through $x_0$, but in the end what we end up learning is to predict the noise in a noisy image and not the mean of the posterior backward distribution. Furthermore, one could decide to learn the variance by formulating it as of the posterior backward distribution, but DDPM found that gave unstable training and poorer sample quality, so they opted for fixed variances decided by the scheduler.\n",
    "\n",
    "### Training\n",
    "\\\n",
    "The algorithm for training is straightforward and note that it is the weights of the U-net that are being trained. U-net with self attention is used.\n",
    "\n",
    "![](assets/training_diffusion.png)\n",
    "\n",
    "Time is uniformly sampled and each time step doesn't have any weighting, meaning each time step is equally as important. T is set to 1000. [Katherine Crowson](https://github.com/crowsonkb) recommends to use bigger batch sizes when training diffusion models, usually 512 or higher, and the reasoning is because the inherent noisy process renders gradients generally noisy as well, so to combat this a bigger batch size is desirable. This is unsupervised, so no labels are needed and the ground truth is instead the sampled standard gaussian noise at time t. In a sense training the model in the forward process is teaching it to denoise, because noisy_image - noise = clean_image. Lastly, because of noise perturbation to the data, this can be seen as a form of data augmentation, since each data with perturbed noise is a slightly new kind of data point.\n",
    "\n",
    "### Backward process\n",
    "\\\n",
    "Unlike the forward process where we could just use the diffusion kernel to jump t steps, in the backward process we are stuck with the markovian structure. So we need to go one step at a time denoising the sampled standard gaussian noise until we get a clean image. This process is the inference step and therefore the sampling process, where we use our trained model to predict the noise at time t given a noisy latent. The process can be described in the following image.\n",
    "\n",
    "![](assets/backward_process.png)\n",
    "\n",
    "The backward distribution can be derived with Bayes' rule to be a gaussian.\n",
    "\n",
    "$$q(x_{t-1}|x_t, x_0) = \\dfrac{q(x_t | x_{t-1}, x_0)q(x_{t-1} | x_0)}{q(x_t | x_0)} \\propto \\mathcal{N}\\left(\\underbrace{\\dfrac{\\sqrt{\\alpha_t}(1-\\alpha_{t-1})x_t + \\sqrt{\\alpha_{t-1}}(1-\\alpha_t)x_0}{1-\\alpha_t}}_{\\mu_q(x_t, x_0)}, \\underbrace{\\dfrac{(1-\\alpha_t)(1-\\alpha_{t-1})}{1-\\alpha_t}I}_{\\sum_q(t)}\\right)$$\n",
    "\n",
    "\n",
    "* backward process => move back from out-distribution to in-distribution\n",
    "* backward process when doing sampling everytime is seen as a decision point where you can go from one mode to another (seems to align with score-based formulation)\n",
    "* Diffusion paths, lots of possible latents can pass through a point in the diffusion path\n",
    "* diffusion backward scheduler only takes a small proportion of the predicted noise, so we can move slowly from T to t=0. Each timestep will produce a latent that is slowly denoised\n",
    "* Reverse process continuous but has to convert to discrete before reaching the image x_0, because pixels in an image are discrete, not continuous\n",
    "* Sampler and scheduler is different! Some people use them interchangeably but they are not the same thing!\n",
    "* Backward process mathematically can be seen as sampling $x_{t-1}$ from $q_{\\theta}(x_{t-1}|x_t)$, that is $x_{t-1} \\sim q_{\\theta}(x_{t-1}|x_t)$.\n",
    "* Markov chain is not bypassed in the backward process, so we need to go step by step until we get a clean image.\n",
    "\n",
    "**Conclusion**\n",
    "So just to recape, we have done...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45409b0e11161d91",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Formal walkthrough\n",
    "After giving this section some thought, I don't think I'm able to provide anything of value than what hasn't already been shown in other papers and books. There are three good papers and a book on the diffusion math that covers it from scratch\n",
    "1. [Understanding Diffusion Models: A Unified Perspective](https://arxiv.org/pdf/2208.11970). I like this one alot because it shows literally all the derivation steps. It also shows different objectives of diffusion that all are equal up to a scaling factor. The objectives are mean of posterior for the reverse process, noise in images in the forward process, original image $x_0$ given arbitrary latent $x_t$ and the score function.\n",
    "2. [Tutorial on Diffusion Models for Imaging and Vision](https://arxiv.org/pdf/2403.18103). Also from scratch, but elaborates on the score function formulation more so than the paper above. Also goes into ODE and SDE of diffusion.\n",
    "3. [Step-by-Step Diffusion: An Elementary Tutorial](https://arxiv.org/pdf/2406.08929). Also from scratch, but covers it from a different perspective. Covers DDPM and DDIM. Also covers flow matching, which is a neat technique to know since stable diffusion 3 uses it.\n",
    "4. [Understanding deep learning](https://udlbook.github.io/udlbook/). Chapter 18 covers diffusion models, specifically DDPM using the noise objective, i.e. model learns to predict the noise added to latents. Good breakdown, although two of the formulas used, change of variable in gaussian and product of two gaussians, can feel a bit mysterious. The first paper listed here does give a good derivation of product of two gaussians without using the formula for product of two gaussians, so look at the paper if it's not clear.\n",
    "5. [Original DDPM paper](https://arxiv.org/pdf/2006.11239). Then of course reading the original paper is always good, although they don't usually have the space to explain all the steps or show all the derivations, so it's usually more compressed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
